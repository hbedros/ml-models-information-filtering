{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Bigram Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.4603\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Government News       0.08      0.10      0.09       316\n",
      "    Middle-east       0.12      0.14      0.13       159\n",
      "           News       0.84      0.91      0.87      1821\n",
      "        US_News       0.08      0.08      0.08       160\n",
      "      left-news       0.13      0.12      0.12       897\n",
      "       politics       0.28      0.24      0.26      1344\n",
      "\n",
      "       accuracy                           0.46      4697\n",
      "      macro avg       0.26      0.27      0.26      4697\n",
      "   weighted avg       0.44      0.46      0.45      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Naive Bayes model and dataset\n",
    "from models.naive_bayes_bigram_model import NaiveBayesBigram\n",
    "from utils.utils import fake_df\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb_model = NaiveBayesBigram()\n",
    "\n",
    "# Train the model on the dataset\n",
    "# Assuming `text` is the column with text data and `label` is the column with labels\n",
    "results = nb_model.train(fake_df, text_column='text', label_column='subject')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Naive Bayes Accuracy: {results['accuracy']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(results['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m nb_model \u001b[38;5;241m=\u001b[39m NaiveBayesBigram()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model on the dataset\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Assuming `text` is the column with text data and `label` is the column with labels\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mnb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaive Bayes Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/MSDS/Capstone/CODE/ml-models-information-filtering/src/models/naive_bayes_bigram_model.py:27\u001b[0m, in \u001b[0;36mNaiveBayesBigram.train\u001b[0;34m(self, df, text_column, label_column)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Extract features and labels\u001b[39;00m\n\u001b[1;32m     26\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[text_column])\n\u001b[0;32m---> 27\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_column\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Split into training and testing sets\u001b[39;00m\n\u001b[1;32m     30\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from models.naive_bayes_bigram_model import NaiveBayesBigram\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "file_path = \"local_path/lda_bert_processed_marr_old.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize the Naive Bayes model\n",
    "nb_model = NaiveBayesBigram()\n",
    "\n",
    "# Train the model on the dataset\n",
    "# Assuming `text` is the column with text data and `label` is the column with labels\n",
    "results = nb_model.train(df, text_column='text', label_column='label')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Naive Bayes Accuracy: {results['accuracy']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(results['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['_id', 'filename', 'text', 'contentType', 'topic', 'relevant_to_σι keting ky kxdx kx', 'similarity_to_σι keting ky kxdx kx', 'relevant_to_model regression data figure residuals', 'similarity_to_model regression data figure residuals', 'embedding_0', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4', 'embedding_5', 'embedding_6', 'embedding_7', 'embedding_8', 'embedding_9', 'embedding_10', 'embedding_11', 'embedding_12', 'embedding_13', 'embedding_14', 'embedding_15', 'embedding_16', 'embedding_17', 'embedding_18', 'embedding_19', 'embedding_20', 'embedding_21', 'embedding_22', 'embedding_23', 'embedding_24', 'embedding_25', 'embedding_26', 'embedding_27', 'embedding_28', 'embedding_29', 'embedding_30', 'embedding_31', 'embedding_32', 'embedding_33', 'embedding_34', 'embedding_35', 'embedding_36', 'embedding_37', 'embedding_38', 'embedding_39', 'embedding_40', 'embedding_41', 'embedding_42', 'embedding_43', 'embedding_44', 'embedding_45', 'embedding_46', 'embedding_47', 'embedding_48', 'embedding_49', 'embedding_50', 'embedding_51', 'embedding_52', 'embedding_53', 'embedding_54', 'embedding_55', 'embedding_56', 'embedding_57', 'embedding_58', 'embedding_59', 'embedding_60', 'embedding_61', 'embedding_62', 'embedding_63', 'embedding_64', 'embedding_65', 'embedding_66', 'embedding_67', 'embedding_68', 'embedding_69', 'embedding_70', 'embedding_71', 'embedding_72', 'embedding_73', 'embedding_74', 'embedding_75', 'embedding_76', 'embedding_77', 'embedding_78', 'embedding_79', 'embedding_80', 'embedding_81', 'embedding_82', 'embedding_83', 'embedding_84', 'embedding_85', 'embedding_86', 'embedding_87', 'embedding_88', 'embedding_89', 'embedding_90', 'embedding_91', 'embedding_92', 'embedding_93', 'embedding_94', 'embedding_95', 'embedding_96', 'embedding_97', 'embedding_98', 'embedding_99', 'embedding_100', 'embedding_101', 'embedding_102', 'embedding_103', 'embedding_104', 'embedding_105', 'embedding_106', 'embedding_107', 'embedding_108', 'embedding_109', 'embedding_110', 'embedding_111', 'embedding_112', 'embedding_113', 'embedding_114', 'embedding_115', 'embedding_116', 'embedding_117', 'embedding_118', 'embedding_119', 'embedding_120', 'embedding_121', 'embedding_122', 'embedding_123', 'embedding_124', 'embedding_125', 'embedding_126', 'embedding_127', 'embedding_128', 'embedding_129', 'embedding_130', 'embedding_131', 'embedding_132', 'embedding_133', 'embedding_134', 'embedding_135', 'embedding_136', 'embedding_137', 'embedding_138', 'embedding_139', 'embedding_140', 'embedding_141', 'embedding_142', 'embedding_143', 'embedding_144', 'embedding_145', 'embedding_146', 'embedding_147', 'embedding_148', 'embedding_149', 'embedding_150', 'embedding_151', 'embedding_152', 'embedding_153', 'embedding_154', 'embedding_155', 'embedding_156', 'embedding_157', 'embedding_158', 'embedding_159', 'embedding_160', 'embedding_161', 'embedding_162', 'embedding_163', 'embedding_164', 'embedding_165', 'embedding_166', 'embedding_167', 'embedding_168', 'embedding_169', 'embedding_170', 'embedding_171', 'embedding_172', 'embedding_173', 'embedding_174', 'embedding_175', 'embedding_176', 'embedding_177', 'embedding_178', 'embedding_179', 'embedding_180', 'embedding_181', 'embedding_182', 'embedding_183', 'embedding_184', 'embedding_185', 'embedding_186', 'embedding_187', 'embedding_188', 'embedding_189', 'embedding_190', 'embedding_191', 'embedding_192', 'embedding_193', 'embedding_194', 'embedding_195', 'embedding_196', 'embedding_197', 'embedding_198', 'embedding_199', 'embedding_200', 'embedding_201', 'embedding_202', 'embedding_203', 'embedding_204', 'embedding_205', 'embedding_206', 'embedding_207', 'embedding_208', 'embedding_209', 'embedding_210', 'embedding_211', 'embedding_212', 'embedding_213', 'embedding_214', 'embedding_215', 'embedding_216', 'embedding_217', 'embedding_218', 'embedding_219', 'embedding_220', 'embedding_221', 'embedding_222', 'embedding_223', 'embedding_224', 'embedding_225', 'embedding_226', 'embedding_227', 'embedding_228', 'embedding_229', 'embedding_230', 'embedding_231', 'embedding_232', 'embedding_233', 'embedding_234', 'embedding_235', 'embedding_236', 'embedding_237', 'embedding_238', 'embedding_239', 'embedding_240', 'embedding_241', 'embedding_242', 'embedding_243', 'embedding_244', 'embedding_245', 'embedding_246', 'embedding_247', 'embedding_248', 'embedding_249', 'embedding_250', 'embedding_251', 'embedding_252', 'embedding_253', 'embedding_254', 'embedding_255', 'embedding_256', 'embedding_257', 'embedding_258', 'embedding_259', 'embedding_260', 'embedding_261', 'embedding_262', 'embedding_263', 'embedding_264', 'embedding_265', 'embedding_266', 'embedding_267', 'embedding_268', 'embedding_269', 'embedding_270', 'embedding_271', 'embedding_272', 'embedding_273', 'embedding_274', 'embedding_275', 'embedding_276', 'embedding_277', 'embedding_278', 'embedding_279', 'embedding_280', 'embedding_281', 'embedding_282', 'embedding_283', 'embedding_284', 'embedding_285', 'embedding_286', 'embedding_287', 'embedding_288', 'embedding_289', 'embedding_290', 'embedding_291', 'embedding_292', 'embedding_293', 'embedding_294', 'embedding_295', 'embedding_296', 'embedding_297', 'embedding_298', 'embedding_299', 'embedding_300', 'embedding_301', 'embedding_302', 'embedding_303', 'embedding_304', 'embedding_305', 'embedding_306', 'embedding_307', 'embedding_308', 'embedding_309', 'embedding_310', 'embedding_311', 'embedding_312', 'embedding_313', 'embedding_314', 'embedding_315', 'embedding_316', 'embedding_317', 'embedding_318', 'embedding_319', 'embedding_320', 'embedding_321', 'embedding_322', 'embedding_323', 'embedding_324', 'embedding_325', 'embedding_326', 'embedding_327', 'embedding_328', 'embedding_329', 'embedding_330', 'embedding_331', 'embedding_332', 'embedding_333', 'embedding_334', 'embedding_335', 'embedding_336', 'embedding_337', 'embedding_338', 'embedding_339', 'embedding_340', 'embedding_341', 'embedding_342', 'embedding_343', 'embedding_344', 'embedding_345', 'embedding_346', 'embedding_347', 'embedding_348', 'embedding_349', 'embedding_350', 'embedding_351', 'embedding_352', 'embedding_353', 'embedding_354', 'embedding_355', 'embedding_356', 'embedding_357', 'embedding_358', 'embedding_359', 'embedding_360', 'embedding_361', 'embedding_362', 'embedding_363', 'embedding_364', 'embedding_365', 'embedding_366', 'embedding_367', 'embedding_368', 'embedding_369', 'embedding_370', 'embedding_371', 'embedding_372', 'embedding_373', 'embedding_374', 'embedding_375', 'embedding_376', 'embedding_377', 'embedding_378', 'embedding_379', 'embedding_380', 'embedding_381', 'embedding_382', 'embedding_383']\n",
      "\n",
      "Unique topics found:\n",
      "1. 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"local_path/lda_bert_processed_marr_old.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check what columns you have\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "\n",
    "# Assuming 'topic' column exists based on your earlier message\n",
    "if 'topic' in df.columns:\n",
    "    # Extract all unique topics\n",
    "    unique_topics = df['topic'].dropna().unique()\n",
    "    \n",
    "    print(\"\\nUnique topics found:\")\n",
    "    for idx, topic in enumerate(unique_topics):\n",
    "        print(f\"{idx+1}. {topic}\")\n",
    "else:\n",
    "    print(\"No 'topic' column found in the CSV!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (notebook)",
   "language": "python",
   "name": "notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
